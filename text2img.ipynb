{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample code/data/processed_posts.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 351\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 315\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    313\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(image_folder)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Create bot instance\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m bot \u001b[38;5;241m=\u001b[39m \u001b[43mContentCreationBot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Generate content from multiple URLs\u001b[39;00m\n\u001b[1;32m    318\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://empowerrecoverycenter.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://empowerresidentialwellness.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://benchmarktransitions.com/programs/residential-treatment/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://newperspectivedetox.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m          ]\n",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m, in \u001b[0;36mContentCreationBot.__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')\u001b[39;00m\n\u001b[1;32m     30\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLANGCHAIN_TRACING_V2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_posts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Initialize models and embeddings\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m, in \u001b[0;36mContentCreationBot.load_posts\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_posts\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     68\u001b[0m         posts \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(posts)\n",
      "File \u001b[0;32m~/Desktop/GenAI_projects/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample code/data/processed_posts.json'"
=======
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
>>>>>>> 86adbc349baebe9bd40354c295500741aca4adac
     ]
    }
   ],
   "source": [
    "import os , json\n",
    "import re , io , random\n",
    "import pandas as pd\n",
    "import time , requests\n",
    "from PIL import Image\n",
    "from typing import Optional, Dict, List\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class ContentCreationBot:\n",
    "    def __init__(self, file_path=\"sample code/data/processed_posts.json\"):\n",
    "        # Initialize API keys and FAISS index path\n",
    "        self.openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        self.huggingface_token = os.getenv('HUGGINGFACE_API_TOKEN')\n",
    "        self.langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "        self.faiss_index_path = 'faiss_index' \n",
    "        self.image_folder = 'generated_images'\n",
    "        self.df = None\n",
    "        self.unique_tags = None\n",
    "        os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "        os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "        # os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')\n",
    "        os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "\n",
    "        self.load_posts(file_path)\n",
    "        # Initialize models and embeddings\n",
    "        self.model = ChatOpenAI(temperature=0.7)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def load_or_create_faiss(self, urls: List[str]) -> FAISS:\n",
    "        \"\"\"Load FAISS index if it exists, otherwise create a new one\"\"\"\n",
    "        if os.path.exists(self.faiss_index_path):\n",
    "            print(\"Loading existing FAISS index...\")\n",
    "            self.db = FAISS.load_local(self.faiss_index_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(\"Creating new FAISS index...\")\n",
    "            self.process_urls(urls)\n",
    "            self.db.save_local(self.faiss_index_path)\n",
    "        return self.db\n",
    "\n",
    "\n",
    "    def process_urls(self, urls: List[str]):\n",
    "        \"\"\"Process multiple URLs content and create vector store\"\"\"\n",
    "        all_docs = []\n",
    "        \n",
    "        for url in urls:\n",
    "            loader = WebBaseLoader(url)\n",
    "            data = loader.load()\n",
    "            \n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            docs = splitter.split_documents(data)\n",
    "            all_docs.extend(docs)  # Combine documents from all URLs\n",
    "            \n",
    "        self.db = FAISS.from_documents(all_docs, self.embeddings)\n",
    "    \n",
    "    def load_posts(self, file_path):\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            posts = json.load(f)\n",
    "            self.df = pd.json_normalize(posts)\n",
    "            self.df['length'] = self.df['line_count'].apply(self.categorize_length)\n",
    "            # collect unique tags\n",
    "            all_tags = self.df['tags'].apply(lambda x: x).sum()\n",
    "            self.unique_tags = list(set(all_tags))\n",
    "\n",
    "    def categorize_length(self, line_count):\n",
    "        if line_count < 5:\n",
    "            return \"Short\"\n",
    "        elif 5 <= line_count <= 10:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Long\"\n",
    "\n",
    "    def get_tags(self):\n",
    "        return self.unique_tags\n",
    "    \n",
    "    def get_filtered_posts(self):\n",
    "        # df_filtered = self.df[\n",
    "        #     (self.df['tags'].apply(lambda tags: tag in tags)) &  # Tags contain 'Influencer'\n",
    "        #     (self.df['length'] == length)  # Line count is less than 5\n",
    "        # ]\n",
    "        df_filtered = self.df\n",
    "        return df_filtered.to_dict(orient='records')\n",
    "    \n",
    "    def get_length_str(self,length):\n",
    "        if length == \"Short\":\n",
    "            return \"1 to 5 lines\"\n",
    "        if length == \"Medium\":\n",
    "            return \"6 to 10 lines\"\n",
    "        if length == \"Long\":\n",
    "            return \"11 to 15 lines\"\n",
    "\n",
    "\n",
    "    def get_cap_prompt(self, length, tag):\n",
    "        length_str = self.get_length_str(length)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Generate a professional LinkedIn post for a healthcare company called 'Emend Healthcare'. \n",
    "\n",
    "        1) Topic: {tag}\n",
    "        2) Length: {length_str}\n",
    "        3) Audience: Healthcare professionals, patients, and families looking for rehabilitation services.\n",
    "        4) Focus: Highlight Emend Healthcare's services, including compassionate care, personalized recovery plans, and innovative approaches to rehabilitation.\n",
    "        5) Include a professional tone, engaging call-to-action, and 3-5 relevant hashtags such as #Rehabilitation, #HealthcareInnovation, and #RecoveryJourney.\n",
    "\n",
    "        The script for the generated post should always be in English.\n",
    "        \"\"\"\n",
    "\n",
    "        examples = self.get_filtered_posts()\n",
    "\n",
    "        if len(examples) > 0:\n",
    "            prompt += \"\\n\\nUse the writing style of the following examples:\"\n",
    "\n",
    "        for i, post in enumerate(examples):\n",
    "            post_text = post['text']\n",
    "            prompt += f'\\n\\nExample {i+1}: {post_text}'\n",
    "\n",
    "            if i == 1:  # Use max two samples\n",
    "                break\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def generate_content(self) -> Dict:\n",
    "        \"\"\"Generate social media captions and image prompt\"\"\"\n",
    "        # Get content summary\n",
    "        # docs = self.db.similarity_search(\"main topic key points\", k=2)\n",
    "        # content_text = \" \".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Templates for content generation\n",
    "        social_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Generate platform-specific social media posts with the following requirements:\n",
    "\n",
    "            1. LinkedIn Post:\n",
    "            - Professional tone\n",
    "            - 1000-1300 characters\n",
    "            - Include 3-5 relevant hashtags\n",
    "            - Focus on industry insights and business value\n",
    "\n",
    "            2. Twitter Post:\n",
    "            - Engaging and concise\n",
    "            - Maximum 240 characters\n",
    "            - Include 2-3 trending hashtags\n",
    "            - Use emojis appropriately\n",
    "\n",
    "            3. Facebook Post:\n",
    "            - Conversational tone\n",
    "            - 500-600 characters\n",
    "            - Include 2-3 hashtags\n",
    "            - Focus on community engagement\n",
    "\n",
    "            Format the output in a dictionary with platform headers as keys and content as values respectively.\"\"\"),\n",
    "            (\"human\", \"{text}\")\n",
    "        ])\n",
    "\n",
    "        image_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Create a detailed image generation prompt with the following structure:\n",
    "\n",
    "            Visual style: Soft, welcoming, professional; use a semi-realistic digital art style suitable for LinkedIn posts.\n",
    "\n",
    "            Composition: Centered composition focusing on serene healthcare environments. Include visuals like a peaceful rehabilitation facility, natural surroundings, or individuals practicing mindfulness and recovery.\n",
    "\n",
    "            Colors: Use a calm and professional palette with blues, greens, and whites. Incorporate rgb(255,154,104) and rgb(0,128,128) effectively for branding elements.\n",
    "\n",
    "            Lighting: Natural, soft lighting to evoke a sense of peace, safety, and trust.\n",
    "\n",
    "            Key elements: \n",
    "            - Show a nurturing environment, such as a therapist or counselor interacting with a patient.\n",
    "            - Include subtle branding cues like the company name \"Emend Healthcare\" as part of the ambiance (e.g., on signage or uniforms).\n",
    "            - Highlight wellness activities like yoga, group therapy, or nature walks.\n",
    "            - Make sure to not use any logo.\n",
    "\n",
    "            Avoid: Avoid text overlays, watermarks, logos, or excessive branding that might make the image feel like an advertisement. The focus should be on authenticity and trust.\n",
    "\n",
    "            Make it detailed and specific for AI image generation.\"\"\"),\n",
    "            (\"human\", \"{text}\")\n",
    "        ])\n",
    "\n",
    "        tag = random.choice(self.get_tags())\n",
    "        length = random.choice(['small','medium','large'])\n",
    "        # Generate content\n",
    "        # social_output = self.model.invoke(social_template.format(text=content_text))\n",
    "        social_output = self.model.invoke(self.get_cap_prompt(length=length,tag=tag))\n",
    "        image_output = self.model.invoke(image_template.format(text=social_output.content))\n",
    "\n",
    "        return {\n",
    "            \"social_media_posts\": social_output.content,\n",
    "            \"image_prompt\": image_output.content\n",
    "        }\n",
    "\n",
    "    def generate_image(self, prompt: str, max_retries: int = 3) -> Optional[bytes]:\n",
    "        \"\"\"Generate image from prompt using Hugging Face API\"\"\"\n",
    "        API_URL = \"https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.huggingface_token}\"}\n",
    "        \n",
    "        negative_prompt = \"\"\"Avoid: text, watermarks, low quality, distortion, \n",
    "                            blurry, artificial, poor lighting, dull colors\"\"\"\n",
    "        \n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"num_inference_steps\": 75,  # Increase steps for better detail\n",
    "                \"guidance_scale\": 8.5,     # Strengthen adherence to the prompt\n",
    "                \"width\": 1024,\n",
    "                \"height\": 768  # Slightly adjust dimensions for LinkedIn-friendly aspect ratio\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(API_URL, headers=headers, json=payload)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return response.content\n",
    "                \n",
    "                if response.status_code == 500:\n",
    "                    print(f\"Internal Server Error: {response.text}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"Retrying in 10 seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                        time.sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"Max retries reached. Could not generate image.\")\n",
    "                        return None\n",
    "\n",
    "                print(f\"Error: {response.status_code}, {response.text}\")\n",
    "                return None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(10)\n",
    "                    continue\n",
    "                return None\n",
    "\n",
    "\n",
    "    def save_image(self, image_bytes: Optional[bytes], caption: str) -> Optional[Image.Image]:\n",
    "        \"\"\"Save the generated image with the timestamp as the filename\"\"\"\n",
    "        if image_bytes is None:\n",
    "            return None        \n",
    "        try:\n",
    "            # Ensure the folder exists\n",
    "            if not os.path.exists(self.image_folder):\n",
    "                os.makedirs(self.image_folder)\n",
    "            \n",
    "            # Get the current timestamp for the filename\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            \n",
    "            # Sanitize the caption for the filename\n",
    "            # sanitized_caption = self.sanitize_filename(caption)\n",
    "            \n",
    "            # Combine timestamp and sanitized caption for the filename\n",
    "            company_name = \"Emend Healthcare\"\n",
    "            filename = f\"{timestamp}_{company_name.replace(' ', '_')}.png\"\n",
    "            file_path = os.path.join(self.image_folder, filename)\n",
    "            \n",
    "            # Save the image\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image.save(file_path)\n",
    "            print(f\"Image saved to {file_path}\")\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def create_full_content(self, urls: List[str]) -> Dict:\n",
    "        \"\"\"Generate complete content package including posts, prompt, and image\"\"\"\n",
    "        try:\n",
    "            # Load or create FAISS index\n",
    "            print(\"Loading or creating FAISS index...\")\n",
    "            self.load_or_create_faiss(urls)\n",
    "            \n",
    "            # Generate content\n",
    "            print(\"Generating content...\")\n",
    "            content = self.generate_content()\n",
    "            \n",
    "            # Generate image\n",
    "            print(\"Generating image...\")\n",
    "            image_bytes = self.generate_image(content[\"image_prompt\"])\n",
    "            print('image Generated')\n",
    "            \n",
    "            # Save image with caption as filename\n",
    "            print(\"Saving image with caption as filename...\")\n",
    "            image = self.save_image(image_bytes, content[\"social_media_posts\"])\n",
    "            \n",
    "            return {\n",
    "                \"social_media_posts\": content[\"social_media_posts\"],\n",
    "                \"image_prompt\": content[\"image_prompt\"],\n",
    "                \"image\": image\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in content creation: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Usage example\n",
    "# Usage example\n",
    "def main():\n",
    "    image_folder = 'generated_images'  # Folder to save images\n",
    "    if not os.path.exists(image_folder):\n",
    "        os.mkdir(image_folder)\n",
    "    # Create bot instance\n",
    "    bot = ContentCreationBot()\n",
    "    \n",
    "    # Generate content from multiple URLs\n",
    "    urls = [\"https://empowerrecoverycenter.com\",\n",
    "            \"https://empowerresidentialwellness.com\",\n",
    "            \"https://benchmarktransitions.com/programs/residential-treatment/\",\n",
    "            \"https://newperspectivedetox.com\"\n",
    "             ]\n",
    "    result = bot.create_full_content(urls)\n",
    "\n",
    "    if result:\n",
    "        print(\"\\nSOCIAL MEDIA POSTS:\")\n",
    "        print(\"-\" * 50)\n",
    "        # posts = json.loads(result['social_media_posts'])\n",
    "        # for k , v in posts.items():\n",
    "        #     print(f\"{k}\\n{v}\")\n",
    "        print(result['social_media_posts'])\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        \n",
    "        print(\"\\nIMAGE PROMPT:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(result[\"image_prompt\"])\n",
    "        \n",
    "        if result[\"image\"]:\n",
    "            print(\"\\nImage generated and saved successfully!\")\n",
    "            \n",
    "            # If in Jupyter notebook, display the image\n",
    "            try:\n",
    "                from IPython.display import display\n",
    "                display(result[\"image\"])\n",
    "            except ImportError:\n",
    "                pass\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.11"
=======
   "version": "3.9.6"
>>>>>>> 86adbc349baebe9bd40354c295500741aca4adac
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
